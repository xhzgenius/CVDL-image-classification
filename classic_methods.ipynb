{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-05-12T13:05:08.253197Z","iopub.status.busy":"2023-05-12T13:05:08.252822Z","iopub.status.idle":"2023-05-12T13:05:08.258679Z","shell.execute_reply":"2023-05-12T13:05:08.257761Z","shell.execute_reply.started":"2023-05-12T13:05:08.253167Z"},"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"markdown","metadata":{},"source":["# 以下为正文！"]},{"cell_type":"markdown","metadata":{},"source":["## 读数据集"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-12T13:07:29.556650Z","iopub.status.busy":"2023-05-12T13:07:29.555990Z","iopub.status.idle":"2023-05-12T13:07:29.562912Z","shell.execute_reply":"2023-05-12T13:07:29.562033Z","shell.execute_reply.started":"2023-05-12T13:07:29.556614Z"},"trusted":true},"outputs":[],"source":["import os\n","from tqdm import tqdm\n","from dataset import MyDataset\n","data_path = \"dataset\\seg_train\"\n","test_path = \"dataset\\seg_test\"\n","dataset = MyDataset(data_path)\n","testset = MyDataset(test_path)"]},{"cell_type":"markdown","metadata":{},"source":["## SIFT"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-12T13:05:08.261125Z","iopub.status.busy":"2023-05-12T13:05:08.260271Z","iopub.status.idle":"2023-05-12T13:05:08.269601Z","shell.execute_reply":"2023-05-12T13:05:08.268439Z","shell.execute_reply.started":"2023-05-12T13:05:08.261092Z"},"trusted":true},"outputs":[],"source":["import cv2\n","\n","sift = cv2.SIFT_create()\n","def sift_algorithm(image: cv2.Mat) -> np.ndarray:\n","    \n","    # 改为灰度图\n","    image_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","    \n","    # 创建SIFT对象实例\n","    \n","    # sift.detect()作用：在图像中找到关键点。\n","    # 如果只想搜索图像的一部分，可以传递mask。\n","    # 每个关键点都是一个特殊的结构，它有许多属性，比如（x，y）坐标、有意义邻域的大小、指定其方向的角度、指定关键点强度的响应等。\n","    # kp = sift.detect(gray, None)\n","\n","    # 现在已经找到关键点，就可以使用sift.compute()根据关键点计算描述符。\n","    # 最终得到的kp是一个关键点元组，des是一个形状（关键点数量）×128的numpy数组。\n","    # kp, des = sift.compute(gray, kp)\n","\n","    # 刚刚我们分两步，先找到关键点、再计算描述符。\n","    # 可以直接一步实现，即使用sift.detectAndCompute()方法。\n","    kp, des = sift.detectAndCompute(image_gray, None)\n","    return des\n","\n","from sklearn.cluster import KMeans\n","import random\n","def get_sift_clustering() -> KMeans:\n","    '''\n","    根据上述计算得出的所有特征点和特征向量，训练一个聚类分类器。\n","    '''\n","    keypoint_vectors = []\n","    for image, label in tqdm(dataset):\n","        vectors = sift_algorithm(image)\n","        if vectors is None: # 可能为None\n","            continue\n","        for v in vectors:\n","            keypoint_vectors.append(v)\n","    random.shuffle(keypoint_vectors)\n","    print(\"Total key points:\", len(keypoint_vectors))\n","    sift_clustering = KMeans(n_clusters=100)\n","    sift_clustering.fit(keypoint_vectors[:10000])\n","    return sift_clustering\n","\n","def get_wordbag_vector(image: cv2.Mat, clustering: KMeans) -> np.ndarray:\n","    '''\n","    用聚类算法给所有特征点分类，然后用词袋模型得到单张图片的词袋向量。\n","    '''\n","    vectors = sift_algorithm(image)\n","    feature_vector = np.zeros(100)\n","    if vectors is None: # 可能为None\n","        return feature_vector\n","    labels = clustering.predict(np.asarray(vectors, dtype=float))\n","    for label in labels:\n","        feature_vector[label] += 1\n","    return feature_vector\n","        "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Test\n","from utils import label_int2str\n","image, label = dataset[0]\n","features = sift_algorithm(image).flatten()\n","print(\"Label: %s (%s), features:\"%(label, label_int2str(label)))\n","print(features)\n","print(\"Shape:\", features.shape)"]},{"cell_type":"markdown","metadata":{},"source":["## Hog"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["winSize = (8,8)\n","blockSize = (8,8)\n","blockStride = (4,4)\n","cellSize = (8,8)\n","nbins = 5\n","\n","hog = cv2.HOGDescriptor(winSize, blockSize, blockStride, cellSize, nbins)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Test\n","image, label = dataset[0]\n","print(image.shape)\n","features = hog.compute(image)\n","print(\"Label: %s (%s), features:\"%(label, label_int2str(label)))\n","print(features)\n","print(\"Shape:\", features.shape)"]},{"cell_type":"markdown","metadata":{},"source":["## Preprocess data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X_sift_train = []\n","X_hog_train = []\n","y_train = []\n","\n","sift_clustering = get_sift_clustering()\n","import pickle\n","pickle.dump(sift_clustering, open(\"./outputs/sift_clustering\", 'wb'))\n","\n","for image, label in tqdm(dataset):\n","    X_sift_train.append(get_wordbag_vector(image, sift_clustering))\n","    X_hog_train.append(hog.compute(image))\n","    y_train.append(label)\n","X_sift_train = np.stack(X_sift_train)\n","X_hog_train = np.stack(X_hog_train)\n","y_train = np.array(y_train)\n","os.makedirs(\"./dataset/processed\", exist_ok=True)\n","np.save(\"./dataset/processed/X_sift_train.npy\", X_sift_train)\n","np.save(\"./dataset/processed/X_hog_train.npy\", X_hog_train)\n","np.save(\"./dataset/processed/y_train.npy\", y_train)\n","\n","X_sift_test = []\n","X_hog_test = []\n","y_test = []\n","for image, label in tqdm(dataset):\n","    X_sift_test.append(get_wordbag_vector(image, sift_clustering))\n","    X_hog_test.append(hog.compute(image))\n","    y_test.append(label)\n","X_sift_test = np.stack(X_sift_test)\n","X_hog_test = np.stack(X_hog_test)\n","y_test = np.array(y_test)\n","os.makedirs(\"./dataset/processed\", exist_ok=True)\n","np.save(\"./dataset/processed/X_sift_test.npy\", X_sift_test)\n","np.save(\"./dataset/processed/X_hog_test.npy\", X_hog_test)\n","np.save(\"./dataset/processed/y_test.npy\", y_test)"]},{"cell_type":"markdown","metadata":{},"source":["## Classifiers"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.svm import LinearSVC, SVC\n","\n","USED_EXTRACTOR = \"sift\"\n","if USED_EXTRACTOR==\"hog\":\n","    X_train_used = X_hog_train\n","    X_test_used = X_hog_test\n","elif USED_EXTRACTOR==\"sift\":\n","    X_train_used = X_sift_train\n","    X_test_used = X_sift_test\n","else:\n","    raise ValueError(\"Either hog or sift! \")\n","print(X_train_used.shape)\n","\n","# X_train_used = np.load(\"./dataset/processed/X_sift_train.npy\")\n","# X_test_used = np.load(\"./dataset/processed/X_sift_test.npy\")\n","# For emergency. \n","\n","# SVM\n","svm = LinearSVC()\n","svm.fit(X_train_used, y_train)\n","print(\"SVM trained. \")\n","svm_accuracy = svm.score(X_test_used, y_test)\n","print(\"Accuracy:\", svm_accuracy)\n","\n","# Kernel-SVM\n","kernel_svm = SVC()\n","kernel_svm.fit(X_train_used, y_train)\n","print(\"Kernel-SVM trained. \")\n","kernel_svm_accuracy = kernel_svm.score(X_test_used, y_test)\n","print(\"Accuracy:\", kernel_svm_accuracy)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.cluster import KMeans\n","\n","# k-means\n","k_means = KMeans(n_clusters=6)\n","k_means.fit(X_train_used)\n","print(\"K-Means trained. \")\n","from utils import clustering_accuracy\n","k_means_predictions = k_means.predict(X_test_used)\n","clustering_accuracy(y_test, k_means_predictions)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Save models\n","import pickle\n","pickle.dump(svm, open(\"./outputs/svm_%s\"%USED_EXTRACTOR, 'wb'))\n","pickle.dump(kernel_svm, open(\"./outputs/kernel_svm_%s\"%USED_EXTRACTOR, 'wb'))\n","pickle.dump(k_means, open(\"./outputs/k_means_%s\"%USED_EXTRACTOR, 'wb'))"]},{"cell_type":"markdown","metadata":{},"source":["## 以上是预处理数据及训练模型过程。以下仅检验模型正确率。"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy of SIFT & SVM: 0.5571285571285571\n","Accuracy of SIFT & Kernel-SVM: 0.7277992277992278\n","Accuracy of SIFT & k-means: 0.33697983697983697\n","Accuracy of HOG & SVM: 0.7166452166452166\n","Accuracy of HOG & Kernel-SVM: 0.8787358787358788\n","Accuracy of HOG & k-means: 0.389031889031889\n"]}],"source":["# Load models\n","import pickle\n","svm_sift = pickle.load(open(\"./outputs/svm_sift\", 'rb'))\n","kernel_svm_sift = pickle.load(open(\"./outputs/kernel_svm_sift\", 'rb'))\n","k_means_sift = pickle.load(open(\"./outputs/k_means_sift\", 'rb'))\n","svm_hog = pickle.load(open(\"./outputs/svm_hog\", 'rb'))\n","kernel_svm_hog = pickle.load(open(\"./outputs/kernel_svm_hog\", 'rb'))\n","k_means_hog = pickle.load(open(\"./outputs/k_means_hog\", 'rb'))\n","\n","# sift_clustering = pickle.load(open(\"./outputs/sift_clustering\", 'rb'))\n","\n","# Load processed data\n","import numpy as np\n","X_sift_test = np.load(\"./dataset/processed/X_sift_test.npy\")\n","X_hog_test = np.load(\"./dataset/processed/X_hog_test.npy\")\n","y_test = np.load(\"./dataset/processed/y_test.npy\")\n","\n","\n","# Test accuracies\n","from utils import clustering_accuracy\n","print(\"Accuracy of SIFT & SVM:\", svm_sift.score(X_sift_test, y_test))\n","print(\"Accuracy of SIFT & Kernel-SVM:\", kernel_svm_sift.score(X_sift_test, y_test))\n","print(\"Accuracy of SIFT & k-means:\", clustering_accuracy(y_test, k_means_sift.predict(X_sift_test)))\n","print(\"Accuracy of HOG & SVM:\", svm_hog.score(X_hog_test, y_test))\n","print(\"Accuracy of HOG & Kernel-SVM:\", kernel_svm_hog.score(X_hog_test, y_test))\n","print(\"Accuracy of HOG & k-means:\", clustering_accuracy(y_test, k_means_hog.predict(X_hog_test)))\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3.7.13 ('ML')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.13"},"vscode":{"interpreter":{"hash":"8809126b2e3f6bd67afd8dec0aaf136102c3339cf179547b748c69a78a732e29"}}},"nbformat":4,"nbformat_minor":4}
