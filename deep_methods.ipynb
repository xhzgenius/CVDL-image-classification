{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 以下是神经网络方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA availability: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import SGD, Adagrad, Adam\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.models import (ResNet18_Weights, ResNet50_Weights,\n",
    "                                VGG11_Weights, resnet18, resnet50, vgg11)\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dataset import MyDataset\n",
    "from my_mlp import MyMLP\n",
    "from utils import label_int2str, label_str2int, save_config\n",
    "\n",
    "\n",
    "device = \"cuda\"\n",
    "assert torch.cuda.is_available(), \"你TM连CUDA都没你跑个JB\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 运行设置："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    # Run name\n",
      "    run_name = \"2023-05-14 21.22.16\"\n",
      "\n",
      "    # Network architecture\n",
      "    # mlp, resnet18, resnet50, vgg11\n",
      "    model_name = \"resnet18\"\n",
      "    model_is_pretrained = False\n",
      "    num_classes = 6\n",
      "\n",
      "    # Optimizer\n",
      "    # SGD, Adagrad, Adam\n",
      "    optimizer_name = \"Adam\"\n",
      "    lr = 0.001000\n",
      "\n",
      "    # Data augmentation\n",
      "    augmentation = \"none\"\n",
      "\n",
      "    # Regularization\n",
      "    regularization = \"none\"\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Run name\n",
    "run_name = datetime.now().strftime(\"%Y-%m-%d %H.%M.%S\")\n",
    "\n",
    "# Network architecture\n",
    "# mlp, resnet18, resnet50, vgg11\n",
    "model_name = \"resnet18\"\n",
    "model_is_pretrained = False\n",
    "num_classes = 6\n",
    "\n",
    "# Optimizer\n",
    "# SGD, Adagrad, Adam\n",
    "optimizer_name = \"Adam\"\n",
    "lr = 1e-3\n",
    "\n",
    "# Data augmentation\n",
    "augmentation = \"none\"\n",
    "\n",
    "# Regularization\n",
    "regularization = \"none\"\n",
    "\n",
    "save_config(run_name, model_name, model_is_pretrained, num_classes, optimizer_name, lr, augmentation, regularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型网络架构：\n",
    "\n",
    "(Reference from pytorch.org)\n",
    "\n",
    "| Weight |Acc@1 |Acc@5 |Params |GFLOPS|\n",
    "| ---- | ---- | ---- | ---- | ---- |\n",
    "| ResNet18_Weights.IMAGENET1K_V1 | 69.758 | 89.078 | 11.7M | 1.81 |\n",
    "| ResNet50_Weights.IMAGENET1K_V2 | 80.858 | 95.434 | 25.6M | 4.09 |\n",
    "| VGG11_Weights.IMAGENET1K_V1 | 69.02 | 88.628 | 132.9M | 7.61 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=6, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "if model_name==\"MLP\":\n",
    "    model = MyMLP(150*150, num_classes)\n",
    "    input_size = (150, 150)\n",
    "elif model_name==\"resnet18\":\n",
    "    if model_is_pretrained:\n",
    "        model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "    else:\n",
    "        model = resnet18(weights=None)\n",
    "    input_size = (224, 224)\n",
    "    model.fc = torch.nn.Linear(512, num_classes)\n",
    "elif model_name==\"resnet50\":\n",
    "    if model_is_pretrained:\n",
    "        model = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "    else:\n",
    "        model = resnet50(weights=None)\n",
    "    input_size = (224, 224)\n",
    "    model.fc = torch.nn.Linear(512, num_classes)\n",
    "elif model_name==\"vgg11\":\n",
    "    if model_is_pretrained:\n",
    "        model = vgg11(weights=VGG11_Weights.DEFAULT)\n",
    "    else:\n",
    "        model = vgg11(weights=None)\n",
    "    input_size = (224, 224)\n",
    "    model.classifier[6] = torch.nn.Linear(4096,num_classes)\n",
    "\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 优化器："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if optimizer_name==\"SGD\":\n",
    "    optimizer = SGD(model.parameters(), lr=lr)\n",
    "elif optimizer_name==\"Adagrad\":\n",
    "    optimizer = Adagrad(model.parameters(), lr=lr)\n",
    "elif optimizer_name==\"Adam\":\n",
    "    optimizer = Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据增强与预处理："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if augmentation==\"none\":\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(input_size), \n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "elif augmentation==\"flip\":\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(input_size), \n",
    "        transforms.RandomHorizontalFlip(), \n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "elif augmentation==\"crop\":\n",
    "    transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(input_size), \n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "elif augmentation==\"norm\":    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(input_size), \n",
    "        transforms.Normalize(mean=(0, 0, 0), std=(1, 1, 1)), \n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "elif augmentation==\"all\":\n",
    "    transform = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(), \n",
    "        transforms.RandomResizedCrop(input_size), \n",
    "        transforms.Normalize(mean=(0, 0, 0), std=(1, 1, 1)), \n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "dataset = datasets.ImageFolder(\"./dataset/seg_train/\", transform)\n",
    "testset = datasets.ImageFolder(\"./dataset/seg_test/\") # Do not transform input images in testing. \n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=8)\n",
    "testloader = DataLoader(testset, batch_size=32, shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练过程："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1: 100%|██████████| 439/439 [00:28<00:00, 15.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 (14034 instances), loss = 0.927014, accuracy = 0.639091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 2: 100%|██████████| 439/439 [00:25<00:00, 17.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 (14034 instances), loss = 0.640746, accuracy = 0.763075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 3: 100%|██████████| 439/439 [00:25<00:00, 17.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 (14034 instances), loss = 0.546315, accuracy = 0.801910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 4: 100%|██████████| 439/439 [00:25<00:00, 16.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 (14034 instances), loss = 0.493939, accuracy = 0.822502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 5: 100%|██████████| 439/439 [00:25<00:00, 17.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 (14034 instances), loss = 0.457830, accuracy = 0.835899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 6: 100%|██████████| 439/439 [00:25<00:00, 16.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 (14034 instances), loss = 0.413728, accuracy = 0.854710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 7: 100%|██████████| 439/439 [00:25<00:00, 16.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 (14034 instances), loss = 0.394481, accuracy = 0.857204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 8: 100%|██████████| 439/439 [00:25<00:00, 16.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 (14034 instances), loss = 0.351716, accuracy = 0.876728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 9:  51%|█████▏    | 226/439 [00:16<00:09, 22.51it/s]"
     ]
    }
   ],
   "source": [
    "def train_one_epoch(model: torch.nn.Module, dataloader: DataLoader, transform: transforms.Compose, \n",
    "                    optimizer: torch.optim.Optimizer, current_epoch: int):\n",
    "    correct_predictions = 0\n",
    "    epoch_instance_count = 0\n",
    "    epoch_loss = 0\n",
    "    for inputs, labels in tqdm(dataloader, desc=\"Training epoch %d\"%current_epoch):\n",
    "        optimizer.zero_grad()\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = torch.nn.functional.cross_entropy(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()*inputs.size(0)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        correct_predictions += torch.sum(predictions==labels.data)\n",
    "        epoch_instance_count += inputs.size(0)\n",
    "    epoch_accuracy = correct_predictions/epoch_instance_count\n",
    "    epoch_loss /= epoch_instance_count\n",
    "    if current_epoch%1==0:\n",
    "        print(\"Epoch %d (%d instances), loss = %f, accuracy = %f\"%(current_epoch, epoch_instance_count, epoch_loss, epoch_accuracy))\n",
    "    if current_epoch%10==0:\n",
    "        torch.save(model, \"./outputs/runs/%s/model_epoch_%d\"%(run_name, current_epoch))\n",
    "    return epoch_accuracy, epoch_loss\n",
    "\n",
    "def test(model: torch.nn.Module, dataloader: DataLoader):\n",
    "    correct_predictions = 0\n",
    "    epoch_instance_count = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc=\"Testing\"):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            correct_predictions += torch.sum(predictions==labels.data)\n",
    "            epoch_instance_count += inputs.size(0)\n",
    "    epoch_accuracy = correct_predictions/epoch_instance_count\n",
    "    print(\"Testing (%d instances), accuracy = %f\"%(epoch_instance_count, epoch_accuracy))\n",
    "\n",
    "epoch_accuracies = []\n",
    "epoch_losses = []\n",
    "model.train()\n",
    "for epoch in range(1, 30):\n",
    "    train_one_epoch(model, dataloader, transform, optimizer, epoch)\n",
    "test(model, testloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('ML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8809126b2e3f6bd67afd8dec0aaf136102c3339cf179547b748c69a78a732e29"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
